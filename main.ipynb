{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data (just deal with level a for now)\n",
    "data = pd.read_csv('OLIDv1.0/olid-training-v1.0.tsv', sep='\\t', header=0, names=['id', 'tweet', 'subtask_a', 'subtask_b', 'subtask_c'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-Level LSTM\n",
    "## Preprocessing and Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import itertools\n",
    "\n",
    "def preprocess(tweet):\n",
    "    # TODO: handles are comming up at @ user instead of @user, need to fix that\n",
    "    # remove hashtags\n",
    "    tweet = ' '.join(re.sub(\"(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    # remove non-ascii characters\n",
    "    tweet = tweet.encode(\"ascii\", \"ignore\").decode()\n",
    "    # remove punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    # lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # replace emoji with text rep\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    # standardizing words\n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    return tweet\n",
    "\n",
    "data.tweet = [preprocess(tweet) for tweet in data.tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.tweet[922])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_df = data[data.subtask_a == 'OFF'].drop(['subtask_b', 'subtask_c'], axis=1)\n",
    "not_df = data[data.subtask_a == 'NOT'].drop(['subtask_b', 'subtask_c'], axis=1)\n",
    "not_df.tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating class imbalance\n",
    "import seaborn as sns\n",
    "print(data.subtask_a.value_counts())\n",
    "ax = sns.countplot(x='subtask_a', data=data, label='Offensive Language', palette='deep')\n",
    "ax.set(xlabel='Tweets', ylabel='Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## downsampling\n",
    "not_df = not_df.sample(n=len(off_df), random_state=12)\n",
    "print(not_df.shape, off_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = off_df.append(not_df).reset_index(drop=True)\n",
    "sns.countplot(df.subtask_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length column for each text\n",
    "df['text_length'] = df.tweet.apply(len)\n",
    "\n",
    "# get average char length by  label types\n",
    "labels = df.groupby('subtask_a').mean()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'] = df['subtask_a'].map({'OFF': 1, 'NOT':0})\n",
    "label = df.type.values\n",
    "\n",
    "# split into train/dev\n",
    "train, dev = sklearn.model_selection.train_test_split(df, test_size=0.1, random_state=0) \n",
    "dev = dev.reset_index(drop=True)\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "lstm_train_seqs = train.tweet\n",
    "lstm_train_labs = train.type\n",
    "\n",
    "lstm_train_labs = to_categorical(train.type)\n",
    "\n",
    "lstm_dev_seqs = dev.tweet\n",
    "lstm_dev_labs = dev.type\n",
    "\n",
    "lstm_dev_labs = to_categorical(dev.type)\n",
    "\n",
    "print(lstm_train_labs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Hyperparameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters\n",
    "max_len = 50 # extended tweet char limit\n",
    "trunc_type = \"post\" \n",
    "padding_type = \"post\" \n",
    "oov_tok = \"<OOV>\" \n",
    "vocab_size = 1000 # maximum number of unique tokens hence we can filter out rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading pretrained FastText Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "\n",
    "file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ro.300.vec.gz'))\n",
    "# file = open('wiki-news-300d-1M-subword.vec', 'r')\n",
    "vectors = {}\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    vector = np.array(values[1:], dtype='float32')\n",
    "    vectors[word] = vector\n",
    "\n",
    "vectors = load_vectors('wiki-news-300d-1M-subword.vec')\n",
    "print(len(vectors.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_tokenizer = Tokenizer(num_words = vocab_size, char_level=False, oov_token= oov_tok)\n",
    "word_tokenizer.fit_on_texts(lstm_train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = word_tokenizer.word_index\n",
    "# check how many words \n",
    "tot_word = len(word_index)\n",
    "print('There are %s unique words in training data. ' % tot_word) \n",
    "\n",
    "# Sequencing and padding on training and dev data\n",
    "training_sequences = word_tokenizer.texts_to_sequences(lstm_train_seqs)\n",
    "training_padded = pad_sequences(training_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "dev_sequences = word_tokenizer.texts_to_sequences(lstm_dev_seqs)\n",
    "dev_padded = pad_sequences(dev_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "# Shape of train tensor\n",
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of dev tensor: ', dev_padded.shape)\n",
    "\n",
    "print(training_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "# vocab_size = 500 # As defined earlier\n",
    "embedding_dim = 300\n",
    "drop_value = 0.2 # dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = vectors.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional, TimeDistributed\n",
    "\n",
    "# LSTM model architecture\n",
    "model = Sequential()\n",
    "# char_model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Embedding(len(word_index)+1, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, dropout=drop_value)))\n",
    "# char_model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fitting a dense spam detector model\n",
    "num_epochs = 30\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "history2 = model.fit(training_padded, lstm_train_labs, epochs=num_epochs, validation_data=(dev_padded, lstm_dev_labs),\\\n",
    "                    callbacks =[early_stop], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this still exists\n",
    "char_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('OLIDv1.0/testset-levela.tsv', sep='\\t', header=0, names=['id', 'tweet'])\n",
    "test.tweet = [preprocess(tweet) for tweet in test.tweet]\n",
    "\n",
    "# test.tweet = ruin(test)\n",
    "\n",
    "test_sequences = word_tokenizer.texts_to_sequences(test.tweet)\n",
    "test_padded = pad_sequences(test_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)\n",
    "print(test_padded.shape)\n",
    "test_y = pd.read_csv('OLIDv1.0/labels-levela.csv', sep=',', header=None, names=['id', 'label'])\n",
    "y_true = list(test_y.label.map({'OFF': 1, 'NOT':0}))\n",
    "\n",
    "\n",
    "preds = model.predict(test_padded)#, test_y) \n",
    "# get the max\n",
    "y_hat = [np.argmax(preds[i]) for i in range(len(preds))]\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/dev\n",
    "# NOT DOWNSAMPLED\n",
    "train, dev = sklearn.model_selection.train_test_split(data, test_size=0.1, random_state=0)\n",
    "train.subtask_a.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data into FastText form and saving to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def get_fastText(data):\n",
    "   fastTweet = [word_tokenize('__label__' + label + ' ' + tweet) for label, tweet in zip(data.subtask_a, data.tweet)]\n",
    "   return fastTweet\n",
    "\n",
    "import csv\n",
    "  \n",
    "def make_file(data, output_file, is_test=False):    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        for row in get_fastText(data):\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "make_file(train, 'data/tweets.train')\n",
    "make_file(dev, 'data/tweets.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastText classifier ##\n",
    "import os\n",
    "import fasttext\n",
    "\n",
    "# reloading already trained model\n",
    "# model = fasttext.load_model('models/fasttext.ftz')\n",
    "\n",
    "def train_fasttext(train, dev):\n",
    "    # ###################\n",
    "    hyper_params = {\"lr\": 0.01,\n",
    "                    \"epoch\": 100,\n",
    "                    \"wordNgrams\": 2,\n",
    "                    \"dim\": 20}     \n",
    "\n",
    "    model = fasttext.train_supervised(input=os.path.join('data',train), **hyper_params)\n",
    "    # reduces size of model and saves\n",
    "    # model.quantize(input='tweets.train', qnorm=True, retrain=True, cutoff=100000)\n",
    "    # model.save_model(os.path.join('models/','fasttext_downsampled' + \".ftz\"))\n",
    "    # ###################\n",
    "\n",
    "    model_acc_training_set = model.test(os.path.join('data',train))\n",
    "    model_acc_validation_set = model.test(os.path.join('data',dev))\n",
    "    # DISPLAY ACCURACY OF TRAINED MODEL\n",
    "    text_line = str(hyper_params) + \" \\naccuracy: \" + str(model_acc_training_set[1])  + \"\\nvalidation: \" + str(model_acc_validation_set[1]) + '\\n' \n",
    "    print(text_line)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model_ft = train_fasttext('tweets.train', 'tweets.dev')\n",
    "\n",
    "def get_preds(model, test_data):\n",
    "    # involves some parsing to get just the letters 'OFF' or 'NOT\n",
    "    return[model.predict(tweet)[0][0][-3:] for tweet in test_data.tweet]\n",
    "\n",
    "def evaluate_fasttext(model, adv = False):\n",
    "    test = pd.read_csv('OLIDv1.0/testset-levela.tsv', sep='\\t', header=0, names=['id', 'tweet'])\n",
    "    test.tweet = [preprocess(tweet) for tweet in test.tweet]\n",
    "    if adv:\n",
    "        test.tweet = ruin(test)\n",
    "    \n",
    "    y_hat = get_preds(model, test)\n",
    "    \n",
    "    test_y = pd.read_csv('OLIDv1.0/labels-levela.csv', sep=',', header=None, names=['id', 'label'])\n",
    "    y_true = test_y.label\n",
    "    print(classification_report(y_true, y_hat))\n",
    "    \n",
    "evaluate_fasttext(model_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def mispell(word):\n",
    "    word = list(word)\n",
    "    x = choice(range(4))\n",
    "\n",
    "    if x == 0:\n",
    "    # shuffle\n",
    "        idx = choice(np.arange(len(word)-2))\n",
    "        before = word[:idx]\n",
    "        subset = word[idx:idx+2]\n",
    "        after = word[idx+2:]\n",
    "        subset = subset[::-1]\n",
    "        word = before + subset + after\n",
    "    if x == 1:\n",
    "    # drop letters\n",
    "        idx = choice(np.arange(len(word)))\n",
    "        word = word[:idx] + word[idx+1:]\n",
    "    if x == 2:\n",
    "    # repeat a letter\n",
    "        idx = choice(np.arange(len(word)))\n",
    "        word = word[:idx] + [word[idx]] + word[idx:]\n",
    "    if x == 3:\n",
    "    # add period at random place\n",
    "        idx = choice(np.arange(len(word)))\n",
    "        word = word[:idx] + ['.'] + word[idx:]\n",
    "    return ''.join(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruin(data):\n",
    "    adv = data.copy()\n",
    "    NSFW = set(['fuck', 'damn', 'shit','dumbass', 'ass', \\\n",
    "                'bad', 'moron', 'idiot', 'mean', 'dumb', \\\n",
    "                'communist','terrible', 'cock', 'liberal', \\\n",
    "                'maga', 'democrat', 'conservative', 'trump', \\\n",
    "                'antifa', 'sick', 'toxic'])\n",
    "    count = 0\n",
    "    for ii, sent in enumerate(adv.tweet):\n",
    "        words = sent.split()\n",
    "        for jj, word in enumerate(words):\n",
    "            # there will be some false positives, it's okay\n",
    "            for bad_word in NSFW:\n",
    "                if bad_word in word:\n",
    "                    # also maybe add randomness in here, like a 50/50 chance it does permute\n",
    "                    words[jj] = mispell(word)\n",
    "                    new = ' '.join(words)\n",
    "                    adv.tweet[ii] = new       \n",
    "                    count += 1\n",
    "    print(count) # 7492 subs\n",
    "    return adv.tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_adv, dev_adv = sklearn.model_selection.train_test_split(adv, test_size=0.1, random_state=0)\n",
    "# train_adv.subtask_a.value_counts()\n",
    "\n",
    "# make_file(train_adv, 'data/tweets.adv.train')\n",
    "# make_file(dev_adv, 'data/tweets.adv.dev')\n",
    "\n",
    "# model_adv = train_fasttext('tweets.adv.train', 'tweets.adv.dev')\n",
    "# evaluate_fasttext(model_adv, adv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sentences for the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.predict(\"@user someone should've taken this piece of shit to a volcano\")) # 99.99%\n",
    "print(model_ft.predict(\"@user someone should've taken this piece of sht to a volcano\")) # 74.88 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.predict(\"@user liberaals are all kookoo\")) # 85\n",
    "print(model_ft.predict(\"@user liberals are all kookoo\")) #98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.predict(\"@user @user @user trump is a very sick man he is toxic and mean spirited\")) # 99.6\n",
    "print(model_ft.predict(\"@user @user @user trmup is a very siick man he is toxc and me.an spirited\")) #98.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.predict('are you fucking serious url'))\n",
    "print(model_ft.predict('are you not fucking serious url'))\n",
    "#                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.predict('@user trump kicks dem butt its not so fun'))\n",
    "print(model_ft.predict('@user trump kicks dem butt its so fun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.predict('@user he is competing for worst president again'))\n",
    "print(model_ft.predict('@user he is not competing for worst president again'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.predict('shit i will drown twitter in bullshhit if one of those is banned'))\n",
    "print(model_ft.predict('shit i will not drown twitter in bullshhit if one of those is banned'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
