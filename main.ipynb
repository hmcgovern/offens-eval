{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data (just deal with level a for now)\n",
    "data = pd.read_csv('OLIDv1.0/olid-training-v1.0.tsv', sep='\\t', header=0, names=['id', 'tweet', 'subtask_a', 'subtask_b', 'subtask_c'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-Level LSTM\n",
    "## Preprocessing and Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import itertools\n",
    "\n",
    "def preprocess(tweet):\n",
    "    # TODO: handles are comming up at @ user instead of @user, need to fix that\n",
    "    # remove hashtags\n",
    "    tweet = ' '.join(re.sub(\"(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    # remove non-ascii characters\n",
    "    tweet = tweet.encode(\"ascii\", \"ignore\").decode()\n",
    "    # remove punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    # lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # replace emoji with text rep\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    # standardizing words\n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    return tweet\n",
    "\n",
    "data.tweet = [preprocess(tweet) for tweet in data.tweet]\n",
    "# dev.tweet = [preprocess(tweet) for tweet in dev.tweet]\n",
    "# train.tweet = [preprocess(tweet) for tweet in train.tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*gets period* you are the cause of my\n"
     ]
    }
   ],
   "source": [
    "print(data.tweet[922])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     amazon is investigating chinese employees who ...\n",
       "4     @user @user obama wanted liberals &amp illegal...\n",
       "8                               @user buy more icecream\n",
       "10    @user @user @user its not my fault you support...\n",
       "11    @user whats the difference between and @user o...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_df = data[data.subtask_a == 'OFF'].drop(['subtask_b', 'subtask_c'], axis=1)\n",
    "not_df = data[data.subtask_a == 'NOT'].drop(['subtask_b', 'subtask_c'], axis=1)\n",
    "not_df.tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT    8840\n",
      "OFF    4400\n",
      "Name: subtask_a, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Tweets'), Text(0, 0.5, 'Counts')]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# investigating class imbalance\n",
    "import seaborn as sns\n",
    "print(data.subtask_a.value_counts())\n",
    "ax = sns.countplot(x='subtask_a', data=data, label='Offensive Language', palette='deep')\n",
    "ax.set(xlabel='Tweets', ylabel='Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4400, 3) (4400, 3)\n"
     ]
    }
   ],
   "source": [
    "## downsampling\n",
    "not_df = not_df.sample(n=len(off_df), random_state=12)\n",
    "print(not_df.shape, off_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hope/miniconda3/envs/nlp/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='subtask_a', ylabel='count'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = off_df.append(not_df).reset_index(drop=True)\n",
    "sns.countplot(df.subtask_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtask_a</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT</th>\n",
       "      <td>55105.063409</td>\n",
       "      <td>113.716136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF</th>\n",
       "      <td>54229.596818</td>\n",
       "      <td>125.134318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  text_length\n",
       "subtask_a                           \n",
       "NOT        55105.063409   113.716136\n",
       "OFF        54229.596818   125.134318"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length column for each text\n",
    "df['text_length'] = df.tweet.apply(len)\n",
    "\n",
    "# get average char length by  label types\n",
    "labels = df.groupby('subtask_a').mean()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>text_length</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98777</td>\n",
       "      <td>@user should i share the quotes from notable u...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54306</td>\n",
       "      <td>@user @user hillary why is she on the social s...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97304</td>\n",
       "      <td>@user @user @user @user you are in the spirit ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26792</td>\n",
       "      <td>@user why you ruin shit lol</td>\n",
       "      <td>OFF</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94391</td>\n",
       "      <td>@user @user only liberals sexuality childrens ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  98777  @user should i share the quotes from notable u...       NOT   \n",
       "1  54306  @user @user hillary why is she on the social s...       OFF   \n",
       "2  97304  @user @user @user @user you are in the spirit ...       NOT   \n",
       "3  26792                        @user why you ruin shit lol       OFF   \n",
       "4  94391  @user @user only liberals sexuality childrens ...       NOT   \n",
       "\n",
       "   text_length  type  \n",
       "0           62     0  \n",
       "1          248     1  \n",
       "2           55     0  \n",
       "3           27     1  \n",
       "4           53     0  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'] = df['subtask_a'].map({'OFF': 1, 'NOT':0})\n",
    "label = df.type.values\n",
    "\n",
    "# data['type'] = data['subtask_a'].map({'OFF': 1, 'NOT':0})\n",
    "# label = data.type.values\n",
    "\n",
    "# split into train/dev\n",
    "train, dev = sklearn.model_selection.train_test_split(df, test_size=0.1, random_state=0) \n",
    "dev = dev.reset_index(drop=True)\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "lstm_train_seqs = train.tweet\n",
    "lstm_train_labs = train.type\n",
    "\n",
    "lstm_train_labs = to_categorical(train.type)\n",
    "\n",
    "lstm_dev_seqs = dev.tweet\n",
    "lstm_dev_labs = dev.type\n",
    "\n",
    "lstm_dev_labs = to_categorical(dev.type)\n",
    "\n",
    "print(lstm_train_labs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Hyperparameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters\n",
    "max_len = 50 # extended tweet char limit\n",
    "trunc_type = \"post\" \n",
    "padding_type = \"post\" \n",
    "oov_tok = \"<OOV>\" \n",
    "vocab_size = 1000 # maximum number of unique tokens hence we can filter out rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading pretrained FastText Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "\n",
    "# file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ro.300.vec.gz'))\n",
    "# # file = open('wiki-news-300d-1M-subword.vec', 'r')\n",
    "# vectors = {}\n",
    "# for line in file:\n",
    "#     values = line.split()\n",
    "#     word = values[0].decode('utf-8')\n",
    "#     vector = np.array(values[1:], dtype='float32')\n",
    "#     vectors[word] = vector\n",
    "\n",
    "# def load_vectors(fname):\n",
    "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     data = {}\n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         data[tokens[0]] = map(float, tokens[1:])\n",
    "#     return data\n",
    "\n",
    "# vectors = load_vectors('wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "print(len(vectors.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_tokenizer = Tokenizer(num_words = vocab_size, char_level=False, oov_token= oov_tok)\n",
    "word_tokenizer.fit_on_texts(lstm_train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13781 unique words in training data. \n",
      "Shape of training tensor:  (7920, 50)\n",
      "Shape of dev tensor:  (880, 50)\n",
      "[  2  80  11   1   3   1  55   1 122 431  49   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "word_index = word_tokenizer.word_index\n",
    "# check how many words \n",
    "tot_word = len(word_index)\n",
    "print('There are %s unique words in training data. ' % tot_word) \n",
    "\n",
    "# Sequencing and padding on training and dev data\n",
    "training_sequences = word_tokenizer.texts_to_sequences(lstm_train_seqs)\n",
    "training_padded = pad_sequences(training_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "dev_sequences = word_tokenizer.texts_to_sequences(lstm_dev_seqs)\n",
    "dev_padded = pad_sequences(dev_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "# Shape of train tensor\n",
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of dev tensor: ', dev_padded.shape)\n",
    "\n",
    "print(training_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "# vocab_size = 500 # As defined earlier\n",
    "embedding_dim = 300\n",
    "drop_value = 0.2 # dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = vectors.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 50, 300)           4134600   \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_15  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 4,275,202\n",
      "Trainable params: 140,602\n",
      "Non-trainable params: 4,134,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Modeling \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional, TimeDistributed\n",
    "\n",
    "# LSTM model architecture\n",
    "model = Sequential()\n",
    "# char_model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Embedding(len(word_index)+1, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, dropout=drop_value)))\n",
    "# char_model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7920 samples, validate on 880 samples\n",
      "Epoch 1/30\n",
      "7920/7920 - 11s - loss: 0.6862 - accuracy: 0.5452 - val_loss: 0.6824 - val_accuracy: 0.5716\n",
      "Epoch 2/30\n",
      "7920/7920 - 8s - loss: 0.6570 - accuracy: 0.6140 - val_loss: 0.6686 - val_accuracy: 0.5920\n",
      "Epoch 3/30\n",
      "7920/7920 - 8s - loss: 0.6399 - accuracy: 0.6308 - val_loss: 0.6414 - val_accuracy: 0.6159\n",
      "Epoch 4/30\n",
      "7920/7920 - 8s - loss: 0.6184 - accuracy: 0.6480 - val_loss: 0.6481 - val_accuracy: 0.6136\n",
      "Epoch 5/30\n",
      "7920/7920 - 8s - loss: 0.6042 - accuracy: 0.6698 - val_loss: 0.6271 - val_accuracy: 0.6364\n",
      "Epoch 6/30\n",
      "7920/7920 - 8s - loss: 0.5913 - accuracy: 0.6823 - val_loss: 0.6126 - val_accuracy: 0.6409\n",
      "Epoch 7/30\n",
      "7920/7920 - 8s - loss: 0.5788 - accuracy: 0.6894 - val_loss: 0.6202 - val_accuracy: 0.6466\n",
      "Epoch 8/30\n",
      "7920/7920 - 8s - loss: 0.5692 - accuracy: 0.7072 - val_loss: 0.6111 - val_accuracy: 0.6670\n",
      "Epoch 9/30\n",
      "7920/7920 - 8s - loss: 0.5584 - accuracy: 0.7109 - val_loss: 0.6167 - val_accuracy: 0.6534\n",
      "Epoch 10/30\n",
      "7920/7920 - 8s - loss: 0.5534 - accuracy: 0.7143 - val_loss: 0.6120 - val_accuracy: 0.6432\n",
      "Epoch 11/30\n",
      "7920/7920 - 8s - loss: 0.5393 - accuracy: 0.7292 - val_loss: 0.6274 - val_accuracy: 0.6716\n",
      "Epoch 12/30\n",
      "7920/7920 - 8s - loss: 0.5319 - accuracy: 0.7302 - val_loss: 0.6220 - val_accuracy: 0.6648\n",
      "Epoch 13/30\n",
      "7920/7920 - 8s - loss: 0.5258 - accuracy: 0.7448 - val_loss: 0.6125 - val_accuracy: 0.6648\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fitting a dense spam detector model\n",
    "num_epochs = 30\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "history2 = model.fit(training_padded, lstm_train_labs, epochs=num_epochs, validation_data=(dev_padded, lstm_dev_labs),\\\n",
    "                    callbacks =[early_stop], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 140, 300)          18300     \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 140, 100)          140400    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_13  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 158,902\n",
      "Trainable params: 140,602\n",
      "Non-trainable params: 18,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this still exists\n",
    "char_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860, 50)\n",
      "[[0.6333401  0.36665988]\n",
      " [0.505089   0.494911  ]\n",
      " [0.85827464 0.14172542]\n",
      " [0.5738131  0.42618695]\n",
      " [0.4402488  0.5597513 ]]\n",
      "[0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.70      0.76       620\n",
      "           1       0.45      0.63      0.52       240\n",
      "\n",
      "    accuracy                           0.68       860\n",
      "   macro avg       0.64      0.66      0.64       860\n",
      "weighted avg       0.72      0.68      0.69       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('OLIDv1.0/testset-levela.tsv', sep='\\t', header=0, names=['id', 'tweet'])\n",
    "test.tweet = [preprocess(tweet) for tweet in test.tweet]\n",
    "\n",
    "# test.tweet = ruin(test)\n",
    "\n",
    "test_sequences = word_tokenizer.texts_to_sequences(test.tweet)\n",
    "test_padded = pad_sequences(test_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type)\n",
    "print(test_padded.shape)\n",
    "test_y = pd.read_csv('OLIDv1.0/labels-levela.csv', sep=',', header=None, names=['id', 'label'])\n",
    "y_true = list(test_y.label.map({'OFF': 1, 'NOT':0}))\n",
    "\n",
    "\n",
    "preds = model.predict(test_padded)#, test_y) \n",
    "# get the max\n",
    "y_hat = [np.argmax(preds[i]) for i in range(len(preds))]\n",
    "print(preds[:5])\n",
    "print(y_hat[:5])\n",
    "print(y_true[:5])\n",
    "print(classification_report(y_true, y_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have char_model. don't have to build it again just why did I even train it on the adversarial data, just need to predict on screwed up data\n",
    "# re-get model\n",
    "# then just format the y_hat\n",
    "# LOLOLOLOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Input for the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOT    7939\n",
       "OFF    3977\n",
       "Name: subtask_a, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into train/dev\n",
    "# NOT DOWNSAMPLED\n",
    "train, dev = sklearn.model_selection.train_test_split(data, test_size=0.1, random_state=0)\n",
    "train.subtask_a.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data into FastText form and saving to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def get_fastText(data):\n",
    "   fastTweet = [word_tokenize('__label__' + label + ' ' + tweet) for label, tweet in zip(data.subtask_a, data.tweet)]\n",
    "   return fastTweet\n",
    "\n",
    "import csv\n",
    "  \n",
    "def make_file(data, output_file, is_test=False):    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        for row in get_fastText(data):\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "make_file(train, 'data/tweets.train')\n",
    "make_file(dev, 'data/tweets.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastText classifier ##\n",
    "import os\n",
    "import fasttext\n",
    "\n",
    "# reloading already trained model\n",
    "# model = fasttext.load_model('models/fasttext.ftz')\n",
    "\n",
    "def train_fasttext(train, dev):\n",
    "    # ###################\n",
    "    hyper_params = {\"lr\": 0.01,\n",
    "                    \"epoch\": 100,\n",
    "                    \"wordNgrams\": 2,\n",
    "                    \"dim\": 20}     \n",
    "\n",
    "    model = fasttext.train_supervised(input=os.path.join('data',train), **hyper_params)\n",
    "    # reduces size of model and saves\n",
    "    # model.quantize(input='tweets.train', qnorm=True, retrain=True, cutoff=100000)\n",
    "    # model.save_model(os.path.join('models/','fasttext_downsampled' + \".ftz\"))\n",
    "    # ###################\n",
    "\n",
    "    model_acc_training_set = model.test(os.path.join('data',train))\n",
    "    model_acc_validation_set = model.test(os.path.join('data',dev))\n",
    "    # DISPLAY ACCURACY OF TRAINED MODEL\n",
    "    text_line = str(hyper_params) + \" \\naccuracy: \" + str(model_acc_training_set[1])  + \"\\nvalidation: \" + str(model_acc_validation_set[1]) + '\\n' \n",
    "    print(text_line)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'epoch': 100, 'wordNgrams': 2, 'dim': 20} \n",
      "accuracy: 0.9866565961732124\n",
      "validation: 0.7779456193353474\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.83      0.89      0.86       620\n",
      "         OFF       0.64      0.52      0.57       240\n",
      "\n",
      "    accuracy                           0.78       860\n",
      "   macro avg       0.73      0.70      0.71       860\n",
      "weighted avg       0.77      0.78      0.78       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model = train_fasttext('tweets.train', 'tweets.dev')\n",
    "\n",
    "def get_preds(model, test_data):\n",
    "    # involves some parsing to get just the letters 'OFF' or 'NOT\n",
    "    return[model.predict(tweet)[0][0][-3:] for tweet in test_data.tweet]\n",
    "\n",
    "def evaluate_fasttext(model, adv = False):\n",
    "    test = pd.read_csv('OLIDv1.0/testset-levela.tsv', sep='\\t', header=0, names=['id', 'tweet'])\n",
    "    test.tweet = [preprocess(tweet) for tweet in test.tweet]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if adv:\n",
    "        test.tweet = ruin(test)\n",
    "    \n",
    "    y_hat = get_preds(model, test)\n",
    "    \n",
    "    test_y = pd.read_csv('OLIDv1.0/labels-levela.csv', sep=',', header=None, names=['id', 'label'])\n",
    "    y_true = test_y.label\n",
    "    print(classification_report(y_true, y_hat))\n",
    "    \n",
    "evaluate_fasttext(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moterfucking\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "def mispell(word):\n",
    "    word = list(word)\n",
    "    x = choice(range(4))\n",
    "\n",
    "    if x == 0:\n",
    "    # shuffle\n",
    "        idx = choice(np.arange(len(word)-2))\n",
    "        before = word[:idx]\n",
    "        subset = word[idx:idx+2]\n",
    "        after = word[idx+2:]\n",
    "        subset = subset[::-1]\n",
    "        word = before + subset + after\n",
    "    if x == 1:\n",
    "    # drop letters\n",
    "        idx = choice(np.arange(len(word)))\n",
    "        word = word[:idx] + word[idx+1:]\n",
    "    if x == 2:\n",
    "    # repeat a letter\n",
    "        idx = choice(np.arange(len(word)))\n",
    "        word = word[:idx] + [word[idx]] + word[idx:]\n",
    "    if x == 3:\n",
    "    # add period at random place\n",
    "        idx = choice(np.arange(len(word)))\n",
    "        word = word[:idx] + ['.'] + word[idx:]\n",
    "    return ''.join(word)\n",
    "\n",
    "print(mispell('motherfucking'))\n",
    "\n",
    "# Just do this for a hot sec. \n",
    "# print([mispell(word) for word in NSFW])\n",
    "\n",
    "def negate(word):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify train and dev with some proportion of the set being \n",
    "# adv = data.copy()\n",
    "\n",
    "def ruin(data):\n",
    "    adv = data.copy()\n",
    "    NSFW = set(['fuck', 'damn', 'shit','dumbass', 'ass', \\\n",
    "                'bad', 'moron', 'idiot', 'mean', 'dumb', \\\n",
    "                'communist','terrible', 'cock', 'liberal', \\\n",
    "                'maga', 'democrat', 'conservative', 'trump', \\\n",
    "                'antifa', 'sick', 'toxic'])\n",
    "    count = 0\n",
    "    for ii, sent in enumerate(adv.tweet):\n",
    "        words = sent.split()\n",
    "        for jj, word in enumerate(words):\n",
    "            # there will be some false positives, it's okay\n",
    "            for bad_word in NSFW:\n",
    "                if bad_word in word:\n",
    "                    # also maybe add randomness in here, like a 50/50 chance it does permute\n",
    "                    words[jj] = mispell(word)\n",
    "                    new = ' '.join(words)\n",
    "#                     print()\n",
    "#                     print(adv.subtask_a[ii])\n",
    "                    print(words)\n",
    "                    \n",
    "                    print(new)\n",
    "                    adv.tweet[ii] = new       \n",
    "                    count += 1\n",
    "    print(count) # 7492 subs\n",
    "    return adv.tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dmeocrats', 'support', 'antifa', 'muslim', 'brotherhood', 'ms13', 'isis', 'pedophilia', 'child', 'trafficking', 'taxpayer', 'funded', 'abortions', 'election', 'fraud', 'sedition', 'and', 'treason', '@user', 'url']\n",
      "dmeocrats support antifa muslim brotherhood ms13 isis pedophilia child trafficking taxpayer funded abortions election fraud sedition and treason @user url\n",
      "['dmeocrats', 'support', 'antiffa', 'muslim', 'brotherhood', 'ms13', 'isis', 'pedophilia', 'child', 'trafficking', 'taxpayer', 'funded', 'abortions', 'election', 'fraud', 'sedition', 'and', 'treason', '@user', 'url']\n",
      "dmeocrats support antiffa muslim brotherhood ms13 isis pedophilia child trafficking taxpayer funded abortions election fraud sedition and treason @user url\n",
      "['is', 'revered', 'by', 'con.servatives', 'hated', 'by', 'progressives/socialist/democrats', 'that', 'want', 'to', 'change', 'it']\n",
      "is revered by con.servatives hated by progressives/socialist/democrats that want to change it\n",
      "['is', 'revered', 'by', 'con.servatives', 'hated', 'by', 'progressives/socilist/democrats', 'that', 'want', 'to', 'change', 'it']\n",
      "is revered by con.servatives hated by progressives/socilist/democrats that want to change it\n",
      "['what', 'the', 'uck', 'did', 'he', 'do', 'this', 'time']\n",
      "what the uck did he do this time\n",
      "['the', 'place', 'for', 'fed', 'up', 'con.servatives']\n",
      "the place for fed up con.servatives\n",
      "['so', 'a', 'wild', 'claim', 'from', '36', 'years', 'ago', 'of', 'groping', 'has', 'evolved', 'into', 'a', 'rape', 'a', 'violent', 'sexual', 'event', 'by', 'move', 'url', 'a', 'soros', 'based', 'org', 'that', 'supports', 'blm', 'ant.ifa', 'etc', 'unbeievable']\n",
      "so a wild claim from 36 years ago of groping has evolved into a rape a violent sexual event by move url a soros based org that supports blm ant.ifa etc unbeievable\n",
      "['willie', 'nelson', 'laughs', 'at', 'consrvatives', 'who', 'are', 'upset', 'with', 'him', 'url']\n",
      "willie nelson laughs at consrvatives who are upset with him url\n",
      "['admn', 'matt', 'hardy', 'and', 'randy', 'orton', 'put', 'on', 'one', 'hell', 'in', 'a', 'cell', 'match', 'woo', 'i', 'hope', 'he', 'is', 'okay']\n",
      "admn matt hardy and randy orton put on one hell in a cell match woo i hope he is okay\n",
      "['@', 'all', 'family/friends', 'do', 'not', 'tell', 'me', 'abd', 'shit', 'that', 'your', 'bf/gf', 'did', 'to', 'you', 'just', 'to', 'go', 'right', 'back', 'to', 'them']\n",
      "@ all family/friends do not tell me abd shit that your bf/gf did to you just to go right back to them\n",
      "['@', 'all', 'family/friends', 'do', 'not', 'tell', 'me', 'abd', 'siht', 'that', 'your', 'bf/gf', 'did', 'to', 'you', 'just', 'to', 'go', 'right', 'back', 'to', 'them']\n",
      "@ all family/friends do not tell me abd siht that your bf/gf did to you just to go right back to them\n",
      "['@user', \"'fellow\", 'demorcats', \"don't\", 'want', '@user', '@user', 'to', 'do', 'well', \"it's\", 'not', 'about', \"what's\", 'best', 'for', 'america', \"it's\", 'purely', \"politics'\", 'dov', 'hikind', 'democratic', 'ny', 'assemblyman', 'url']\n",
      "@user 'fellow demorcats don't want @user @user to do well it's not about what's best for america it's purely politics' dov hikind democratic ny assemblyman url\n",
      "['@user', \"'fellow\", 'demorcats', \"don't\", 'want', '@user', '@user', 'to', 'do', 'well', \"it's\", 'not', 'about', \"what's\", 'best', 'for', 'america', \"it's\", 'purely', \"politics'\", 'dov', 'hikind', 'ddemocratic', 'ny', 'assemblyman', 'url']\n",
      "@user 'fellow demorcats don't want @user @user to do well it's not about what's best for america it's purely politics' dov hikind ddemocratic ny assemblyman url\n",
      "['@user', \"'fellow\", 'demorcats', \"don't\", 'want', '@user', '@user', 'to', 'do', 'well', \"it's\", 'not', 'about', \"what's\", 'best', 'for', 'america', \"it's\", 'purely', \"politics'\", 'dov', 'hikind', 'ddemocratic', 'ny', 'asesmblyman', 'url']\n",
      "@user 'fellow demorcats don't want @user @user to do well it's not about what's best for america it's purely politics' dov hikind ddemocratic ny asesmblyman url\n",
      "['is', 'that', 'serious', 'holy', 'shhit', 'please', 'your', 'fucking', 'assholes', \"don't\", 'blame', 'someone', 'for', 'the', 'death', 'of', 'other', 'one', 'she', 'is', 'sad', 'enough', 'for', 'today', \"don't\", 'you', 'see', 'it', \"isn't\", 'fault', 'of', 'none', 'he', 'had', 'an', 'overdose', 'and', 'died', 'end', 'stop', 'wanting', 'someone', 'to', 'blame', 'fuckers']\n",
      "is that serious holy shhit please your fucking assholes don't blame someone for the death of other one she is sad enough for today don't you see it isn't fault of none he had an overdose and died end stop wanting someone to blame fuckers\n",
      "['is', 'that', 'serious', 'holy', 'shhit', 'please', 'your', '.fucking', 'assholes', \"don't\", 'blame', 'someone', 'for', 'the', 'death', 'of', 'other', 'one', 'she', 'is', 'sad', 'enough', 'for', 'today', \"don't\", 'you', 'see', 'it', \"isn't\", 'fault', 'of', 'none', 'he', 'had', 'an', 'overdose', 'and', 'died', 'end', 'stop', 'wanting', 'someone', 'to', 'blame', 'fuckers']\n",
      "is that serious holy shhit please your .fucking assholes don't blame someone for the death of other one she is sad enough for today don't you see it isn't fault of none he had an overdose and died end stop wanting someone to blame fuckers\n",
      "['is', 'that', 'serious', 'holy', 'shhit', 'please', 'your', '.fucking', 'asshooles', \"don't\", 'blame', 'someone', 'for', 'the', 'death', 'of', 'other', 'one', 'she', 'is', 'sad', 'enough', 'for', 'today', \"don't\", 'you', 'see', 'it', \"isn't\", 'fault', 'of', 'none', 'he', 'had', 'an', 'overdose', 'and', 'died', 'end', 'stop', 'wanting', 'someone', 'to', 'blame', 'fuckers']\n",
      "is that serious holy shhit please your .fucking asshooles don't blame someone for the death of other one she is sad enough for today don't you see it isn't fault of none he had an overdose and died end stop wanting someone to blame fuckers\n",
      "['is', 'that', 'serious', 'holy', 'shhit', 'please', 'your', '.fucking', 'asshooles', \"don't\", 'blame', 'someone', 'for', 'the', 'death', 'of', 'other', 'one', 'she', 'is', 'sad', 'enough', 'for', 'today', \"don't\", 'you', 'see', 'it', \"isn't\", 'fault', 'of', 'none', 'he', 'had', 'an', 'overdose', 'and', 'died', 'end', 'stop', 'wanting', 'someone', 'to', 'blame', 'fcukers']\n",
      "is that serious holy shhit please your .fucking asshooles don't blame someone for the death of other one she is sad enough for today don't you see it isn't fault of none he had an overdose and died end stop wanting someone to blame fcukers\n",
      "['classmat', 'says', 'accuser', 'is', 'absolutely', 'nuts\"', 'url', '\"judge', 'kavanaugh\"@user', '@user', '@user', '\"']\n",
      "classmat says accuser is absolutely nuts\" url \"judge kavanaugh\"@user @user @user \"\n",
      "['*voice', 'in', 'my', 'head', 'while', 'i', 'transfer', 'money', 'to', 'my', 'credit', 'card*', 'bbullshit', 'that', 'i', 'bought', 'and', 'have', 'to', 'pay', 'for']\n",
      "*voice in my head while i transfer money to my credit card* bbullshit that i bought and have to pay for\n",
      "['employee', 'resisters\"', 'r', 'looking', 'up', 'conservatievs', 'license', 'plates', 'home', 'addresses', 'legal', 'details', 'in', 'lexisnexis', '2give', 'to', 'dem', 'socialists/antifa', 'project', 'veritas', 'but', \"don't\", 'worry', 'no', 'govt', 'in', 'history', 'has', 'ever', 'made', 'political', 'opponent', 'lists', 'and', 'oppressed', 'them', 'url']\n",
      "employee resisters\" r looking up conservatievs license plates home addresses legal details in lexisnexis 2give to dem socialists/antifa project veritas but don't worry no govt in history has ever made political opponent lists and oppressed them url\n",
      "['employee', 'resisters\"', 'r', 'looking', 'up', 'conservatievs', 'license', 'plates', 'home', 'addresses', 'legal', 'details', 'in', 'lexisnexis', '2give', 'to', 'dem', 'social.ists/antifa', 'project', 'veritas', 'but', \"don't\", 'worry', 'no', 'govt', 'in', 'history', 'has', 'ever', 'made', 'political', 'opponent', 'lists', 'and', 'oppressed', 'them', 'url']\n",
      "employee resisters\" r looking up conservatievs license plates home addresses legal details in lexisnexis 2give to dem social.ists/antifa project veritas but don't worry no govt in history has ever made political opponent lists and oppressed them url\n",
      "['mcparland', 'latest', 'floor', 'crosser', 'is', 'telling', 'truths', 'the', 'libreals', 'dont', 'like', 'hearing', 'url', 'url']\n",
      "mcparland latest floor crosser is telling truths the libreals dont like hearing url url\n",
      "['are', 'you', 'ufcking', 'serious', 'url']\n",
      "are you ufcking serious url\n",
      "['@user', '@user', '@user', 'i', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'turmp', 'and', 'blacks', 'for', 'trump', 'were', 'paid', 'supporters', 'then', 'i', 'said', 'you', 'mean', 'antifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'no', 'they', 'are', 'anti', 'fascist', 'then', 'i', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me']\n",
      "@user @user @user i got in a pretty deep debate with my friend and she told me that latinos for turmp and blacks for trump were paid supporters then i said you mean antifa are paid domestic terrorist and she said no they are anti fascist then i said they are the fascist are you kidding me\n",
      "['@user', '@user', '@user', 'i', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'turmp', 'and', 'blacks', 'for', 'turmp', 'were', 'paid', 'supporters', 'then', 'i', 'said', 'you', 'mean', 'antifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'no', 'they', 'are', 'anti', 'fascist', 'then', 'i', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me']\n",
      "@user @user @user i got in a pretty deep debate with my friend and she told me that latinos for turmp and blacks for turmp were paid supporters then i said you mean antifa are paid domestic terrorist and she said no they are anti fascist then i said they are the fascist are you kidding me\n",
      "['@user', '@user', '@user', 'i', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'turmp', 'and', 'blacks', 'for', 'turmp', 'were', 'paid', 'supporters', 'then', 'i', 'said', 'you', 'men', 'antifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'no', 'they', 'are', 'anti', 'fascist', 'then', 'i', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me']\n",
      "@user @user @user i got in a pretty deep debate with my friend and she told me that latinos for turmp and blacks for turmp were paid supporters then i said you men antifa are paid domestic terrorist and she said no they are anti fascist then i said they are the fascist are you kidding me\n",
      "['@user', '@user', '@user', 'i', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'turmp', 'and', 'blacks', 'for', 'turmp', 'were', 'paid', 'supporters', 'then', 'i', 'said', 'you', 'men', 'natifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'no', 'they', 'are', 'anti', 'fascist', 'then', 'i', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me']\n",
      "@user @user @user i got in a pretty deep debate with my friend and she told me that latinos for turmp and blacks for turmp were paid supporters then i said you men natifa are paid domestic terrorist and she said no they are anti fascist then i said they are the fascist are you kidding me\n",
      "['@user', 'omg', 'is', 'he', 'for', 'real', ')', 'this', 'happened', 'in', 'peru', 'like', '40', 'years', 'ago', 'and', 'the', 'inti', 'devaluated', 'so', 'fukcing', 'much', 'that', 'they', 'had', 'to', 'comoletely', 'change', 'the', 'coin', 'system', 'because', 'our', 'money', 'was', 'worthless', 'i']\n",
      "@user omg is he for real ) this happened in peru like 40 years ago and the inti devaluated so fukcing much that they had to comoletely change the coin system because our money was worthless i\n",
      "[\"it's\", 'not', 'about', 'our', 'disagreements', 'with', 'its', 'that', 'conservaatives', \"can't\", 'debate', 'honestly', 'and', 'they', 'have', 'no', 'integrity', 'whatever', 'gets', 'them', 'thru', 'today', 'is', 'all', 'that', 'matters', 'to', 'them', \"they're\", 'fundamentally', 'dishonest', 'people', 'url']\n",
      "it's not about our disagreements with its that conservaatives can't debate honestly and they have no integrity whatever gets them thru today is all that matters to them they're fundamentally dishonest people url\n",
      "['and', 'apparently', \"i'm\", 'committed', 'to', 'going', 'to', 'a', 'new', 'level', 'since', 'i', 'used', 'the', 'key', 'well', 'uck', 'curiosity', 'killed', 'the', 'cat(hy)']\n",
      "and apparently i'm committed to going to a new level since i used the key well uck curiosity killed the cat(hy)\n",
      "['_international', 'us', 'is', 'in', 'violation', 'of', 'international', 'human', 'rights', 'law', 'by', 'not', 'p.assing', 'stronger', '_control', 'url', 'via', '@user', 'url']\n",
      "_international us is in violation of international human rights law by not p.assing stronger _control url via @user url\n",
      "['are', 'bedfellows', 'with', 'the', 'democrati', 'socialists', 'of', 'america', '(dsa)', 'who', 'has', 'exposed', 'as', 'being', 'inserted', 'throughout', 'government', 'attempting', 'to', 'bring', 'down', 'url']\n",
      "are bedfellows with the democrati socialists of america (dsa) who has exposed as being inserted throughout government attempting to bring down url\n",
      "['barometer', 'labour', 'fails', 'to', 'capitalise', 'as', 'conseravtives', 'fail', 'to', 'please', 'kantar', 'nearly', 'two', 'thirds', 'of', 'the', 'uk', 'believe', 'the', 'government', 'is', 'handling', 'brexit', 'negotations', 'poorly', 'url']\n",
      "barometer labour fails to capitalise as conseravtives fail to please kantar nearly two thirds of the uk believe the government is handling brexit negotations poorly url\n",
      "['@user', 'natifa', 'has', 'ts', 'level', 'influence', \"it's\", 'scary']\n",
      "@user natifa has ts level influence it's scary\n",
      "['@user', 'i', 'have', 'three', 'accounts', 'two', 'quasi', 'anonymous', 'and', 'one', 'completely', 'anon', 'and', 'if', 'twitter', 'ever', 'bans', 'one', 'of', 'the', 'quasi', 'anon', \"i'll\", 'make', 'two', 'anon', 'accounts', 'and', 'tweet', 'the', 'most', 'insane', 's.hit', 'i', 'will', 'drown', 'twitter', 'in', 'bullshit', 'if', 'one', 'of', 'those', 'is', 'banned', \"i'll\", 'make', 'even', 'more']\n",
      "@user i have three accounts two quasi anonymous and one completely anon and if twitter ever bans one of the quasi anon i'll make two anon accounts and tweet the most insane s.hit i will drown twitter in bullshit if one of those is banned i'll make even more\n",
      "['@user', 'i', 'have', 'three', 'accounts', 'two', 'quasi', 'anonymous', 'and', 'one', 'completely', 'anon', 'and', 'if', 'twitter', 'ever', 'bans', 'one', 'of', 'the', 'quasi', 'anon', \"i'll\", 'make', 'two', 'anon', 'accounts', 'and', 'tweet', 'the', 'most', 'insane', 's.hit', 'i', 'will', 'drown', 'twitter', 'in', 'bullshhit', 'if', 'one', 'of', 'those', 'is', 'banned', \"i'll\", 'make', 'even', 'more']\n",
      "@user i have three accounts two quasi anonymous and one completely anon and if twitter ever bans one of the quasi anon i'll make two anon accounts and tweet the most insane s.hit i will drown twitter in bullshhit if one of those is banned i'll make even more\n",
      "['@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', 'city', 'of', 'chicago', 'democr.at', 'run', 'with', 'some', 'of', 'the', 'strictest', 'gun', 'control', 'in', 'the', 'nation', '\"', \"what's\", 'your', 'point', 'url']\n",
      "@user @user @user @user @user @user @user @user @user @user @user @user @user @user city of chicago democr.at run with some of the strictest gun control in the nation \" what's your point url\n",
      "['are', 'so', 'pathetic', 'innocent', 'until', 'proven', 'guilty', 'mea.n', 'much', 'should', 'look', 'at', 'their', 'own', 'party', 'if', 'they', 'need', 'a', 'rapist', 'to', 'hang', 'are', 'straight', 'from', 'the', 'pitts', 'of', 'hell', 'url']\n",
      "are so pathetic innocent until proven guilty mea.n much should look at their own party if they need a rapist to hang are straight from the pitts of hell url\n",
      "['$10', 'million', 'reallocated', 'from', 'fema', 'to', 'ice', 'this', 'guy', 'gives', 'zero', 'shiits', 'about', 'anyone', 'not', 'named', 'trump', 'if', 'youre', 'in', 'the', 'mid', 'atlantic', 'get', 'out', 'now', 'url']\n",
      "$10 million reallocated from fema to ice this guy gives zero shiits about anyone not named trump if youre in the mid atlantic get out now url\n",
      "['$10', 'million', 'reallocated', 'from', 'fema', 'to', 'ice', 'this', 'guy', 'gives', 'zero', 'shiits', 'about', 'anyone', 'not', 'named', 'trum', 'if', 'youre', 'in', 'the', 'mid', 'atlantic', 'get', 'out', 'now', 'url']\n",
      "$10 million reallocated from fema to ice this guy gives zero shiits about anyone not named trum if youre in the mid atlantic get out now url\n",
      "['good', 'thing', 'it', 'was', 'responsible', 'us', 'marines', 'im', 'pretty', 'sure', 'a', 'group', 'of', 'antiffa', 'members', 'wouldnt', 'be', 'running', 'that', 'fast', 'in', 'that', 'direction', 'to', 'help', 'anyone', 'let', 'alone', 'elder', 'seniors', 'thank', 'you', '@user', 'we', 'salute', 'you', '@user', '@user', '@user', 'url']\n",
      "good thing it was responsible us marines im pretty sure a group of antiffa members wouldnt be running that fast in that direction to help anyone let alone elder seniors thank you @user we salute you @user @user @user url\n",
      "['if', 'twitter', 'and', 'facebook', 'can', 'silence', 'conervatives', 'christians', 'we', 'are', 'next', 'once', 'censored', 'the', 'darkness', 'takes', 'over', 'help', 'us', 'grow', 'in', 'by', 'living', 'your', 'faith', 'aloud', 'where', 'we', 'go', 'one', 'we', 'go', 'all', 'prayer', 'warriors', 'please', 'help', 'us', 'grow', 'this', 'movement', 'url']\n",
      "if twitter and facebook can silence conervatives christians we are next once censored the darkness takes over help us grow in by living your faith aloud where we go one we go all prayer warriors please help us grow this movement url\n",
      "['put', 'wallin', 'budget', 'you', 'are', 'going', 'to', 'loose', 'pres', 'trup', 'followers', 'he', 'is', 'why', 'u', 'r', 'there', 'so', 'support', 'him', 'url']\n",
      "put wallin budget you are going to loose pres trup followers he is why u r there so support him url\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hope/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     dmeocrats support antiffa muslim brotherhood m...\n",
       "1     is revered by con.servatives hated by progress...\n",
       "2     @user @user @user @user @user @user @user @use...\n",
       "3     getting the news that she is still up for paro...\n",
       "4     unity demo to oppose the far right in enough i...\n",
       "                            ...                        \n",
       "95             url we have what you are looking for url\n",
       "96    put wallin budget you are going to loose pres ...\n",
       "97    hi @user this is fiona she is about 10 weeks o...\n",
       "98    the conversation about this is so unattractive...\n",
       "99    is so full of herself she is just as painful t...\n",
       "Name: tweet, Length: 100, dtype: object"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruin(test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'epoch': 100, 'wordNgrams': 2, 'dim': 20} \n",
      "accuracy: 0.9869922792883518\n",
      "validation: 0.7507552870090635\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'subtask_a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-6b6b0806e121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets.adv.train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tweets.adv.dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mevaluate_fasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-176-9ffa3b07d99e>\u001b[0m in \u001b[0;36mevaluate_fasttext\u001b[0;34m(model, adv)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-335-f4de9daac04d>\u001b[0m in \u001b[0;36mruin\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmispell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtask_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'subtask_a'"
     ]
    }
   ],
   "source": [
    "train_adv, dev_adv = sklearn.model_selection.train_test_split(adv, test_size=0.1, random_state=0)\n",
    "train_adv.subtask_a.value_counts()\n",
    "\n",
    "make_file(train_adv, 'data/tweets.adv.train')\n",
    "make_file(dev_adv, 'data/tweets.adv.dev')\n",
    "\n",
    "model_adv = train_fasttext('tweets.adv.train', 'tweets.adv.dev')\n",
    "evaluate_fasttext(model_adv, adv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sentences for the chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay well we definitely need ones from the test set, not the trian set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__OFF',), array([0.99990022]))\n",
      "(('__label__OFF',), array([0.74872291]))\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(\"@user someone should've taken this piece of shit to a volcano\")) # 99.99%\n",
    "print(model.predict(\"@user someone should've taken this piece of sht to a volcano\")) # 74.88 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__OFF',), array([0.85089403]))\n",
      "(('__label__OFF',), array([0.98458773]))\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(\"@user liberaals are all kookoo\")) # 85\n",
    "print(model.predict(\"@user liberals are all kookoo\")) #98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__OFF',), array([0.9957692]))\n",
      "(('__label__OFF',), array([0.55787367]))\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(\"@user @user @user trump is a very sick man he is toxic and mean spirited\")) # 99.6\n",
    "print(model.predict(\"@user @user @user trmup is a very siick man he is toxc and me.an spirited\")) #98.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__OFF',), array([0.68974441]))\n",
      "(('__label__NOT',), array([0.68345487]))\n"
     ]
    }
   ],
   "source": [
    "print(model_adv.predict('are you fucking serious url'))\n",
    "print(model_adv.predict('are you not fucking serious url'))\n",
    "#                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__OFF',), array([0.97192186]))\n",
      "(('__label__OFF',), array([0.98201859]))\n"
     ]
    }
   ],
   "source": [
    "print(model_adv.predict('@user trump kicks dem butt its not so fun'))\n",
    "print(model_adv.predict('@user trump kicks dem butt its so fun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__OFF',), array([0.95972419]))\n",
      "(('__label__OFF',), array([0.8462193]))\n"
     ]
    }
   ],
   "source": [
    "print(model_adv.predict('@user he is competing for worst president again'))\n",
    "print(model_adv.predict('@user he is not competing for worst president again'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__NOT',), array([0.93584484]))\n",
      "(('__label__NOT',), array([0.84804094]))\n"
     ]
    }
   ],
   "source": [
    "print(model_adv.predict('shit i will drown twitter in bullshhit if one of those is banned'))\n",
    "print(model_adv.predict('shit i will not drown twitter in bullshhit if one of those is banned'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print(sent.split())\n",
    "# print(adv[adv.tweet.str.count('cock')>1].subtask_a.value_counts())\n",
    "# print([mispell(word) for word in NSFW])\n",
    "# fuck Off: 20, Not: 0\n",
    "# ass Off: 37, Not: 29\n",
    "# damn Off: 0, Not: 1\n",
    "# bad Off: 3, Not: 7\n",
    "# idiot Off: 2, Not : 0\n",
    "# mean Off: 8, Not: 9\n",
    "# dumb Off: 2, Not: 1\n",
    "# terrible Off: 1, Not: 1\n",
    "# cock: Off:1, Not: 0\n",
    "\n",
    "# liberal Off: 65, Not: 74\n",
    "# maga Off: 1, Not: 4 \n",
    "# conservative: Off: 17, Not: 49\n",
    "# trump Off: 29, Not: 44\n",
    "# antifa Off: 10, Not: 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit distance! This is a good way to do this. have an edit distance generator and then you can plot quantitatively by how large the edit distance is. \n",
    "\n",
    "# self-censorship of profanity\n",
    "##  having a set of replacement spellings in a dictionary ('idiot': ['iidiot', 'i.diot'])\n",
    "# sneaky = {'dumbass': 'dumb@$$', \n",
    "# 'pussy': 'pu$$y', \n",
    "# 'ass': 'a$$', \n",
    "# 'fuck': 'f*ck', \n",
    "# 'fucking': 'f*cking', \n",
    "# 'hell':'h*ll', \n",
    "# 'cunt': 'c*nt',\n",
    "# 'hoe': 'h*e',\n",
    "# 'sex': 's*x'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
